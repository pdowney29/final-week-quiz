{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [2,3,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.695359714832659"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.norm(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5369077164427643e-17\n",
      "1.7667112304292042e-17\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import binom\n",
    "X = stats.binom(100, .55) # Declare X to be a binomial random variable\n",
    "print(X.pmf(14))          # P(X = 3)\n",
    "print(X.cdf(14))\n",
    "print(1 - binom.cdf(14, 100, 0.55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7667112304292042e-17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom.cdf(14, 100, 0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 What is the rank of a matrix?\n",
    "In linear algebra, the rank of a matrix {\\displaystyle A} A is the dimension of the vector \n",
    "space generated (or spanned) by its columns.[1] This corresponds to the maximal number of \n",
    "linearly independent columns of {\\displaystyle A} A. This, in turn, is identical to the \n",
    "dimension of the space spanned by its rows.[2] Rank is thus a measure of the \n",
    "\"nondegenerateness\" of the system of linear equations and linear transformation encoded \n",
    "by {\\displaystyle A} A. There are multiple equivalent definitions of rank. \n",
    "A matrix's rank is one of its most fundamental characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Why is rank important for Linear Regression?   Basically, when the columns of your matrix X aren't independent, then the matrix\n",
    "XT∗X\n",
    "can't be inverted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "What is the purpose of the sigmoid function in a Logistic Regression model?  The inverse form\n",
    "of the logistic function is looks kind of like an S, which, I've read, is why it's \n",
    "called a Sigmoid function.  n Logistic Regression, we wish to model a dependent variable(Y) \n",
    "in terms of one or more independent variables(X). It is a method for classification. \n",
    "This algorithm is used for the dependent variable that is Categorical. Y is modeled \n",
    "using a function that gives output between 0 and 1 for\n",
    "all values of X. In Logistic Regression, the Sigmoid (aka Logistic) Function is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6Bias Error\n",
    "Bias are the simplifying assumptions made by a model to make the target function easier to learn.\n",
    "\n",
    "Generally, parametric algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.\n",
    "\n",
    "Low Bias: Suggests less assumptions about the form of the target function.\n",
    "High-Bias: Suggests more assumptions about the form of the target function.\n",
    "Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
    "\n",
    "Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "Bias Error\n",
    "Bias are the simplifying assumptions made by a model to make the target function easier to learn.\n",
    "\n",
    "Generally, parametric algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.\n",
    "\n",
    "Low Bias: Suggests less assumptions about the form of the target function.\n",
    "High-Bias: Suggests more assumptions about the form of the target function.\n",
    "Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
    "\n",
    "Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "Variance is the amount that the estimate of the target function will change if different training data was used.\n",
    "\n",
    "The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to have some variance. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.\n",
    "\n",
    "Machine learning algorithms that have a high variance are strongly influenced by the specifics of the training data. This means that the specifics of the training have influences the number and types of parameters used to characterize the mapping function.\n",
    "\n",
    "Low Variance: Suggests small changes to the estimate of the target function with changes to the training dataset.\n",
    "High Variance: Suggests large changes to the estimate of the target function with changes to the training dataset.\n",
    "Generally, nonparametric machine learning algorithms that have a lot of flexibility have a high variance. For example, decision trees have a high variance, that is even higher if the trees are not pruned before use.\n",
    "\n",
    "Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "\n",
    "Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7  What is a hyperparameter? What part of the dataset (train, validation or test) is responsible for determining this value?\n",
    "Well, most ML models are described by two sets of parameters. The 1st set consists in “regular” parameters that are “learned” through training. The other parameters, called hyperparameters or meta-parameters are parameters which values are set before the learning starts (think, for example, the learning rate, the regularisation parameter, the number of layers or neurons in a layer for ANN etc.)\n",
    "\n",
    "Obviously, different values for those parameters may lead to different (sometimes by a lot) generalisation performance for our Machine Learning model therefore we need to identify a set of optimal values for them and this is done by training multiple models with different values for the hyperparameters (how to chose those values falls under the name of hyperparameter optimisation - Hyperparameter optimization - Wikipedia)\n",
    "\n",
    "Now, imagine you have you data and you need to run a supervised ML algorithm on it. You split the data into:\n",
    "\n",
    "training - this is the data for which your algorithm knows the “labels” and which you will feed it to the training process to build your model.\n",
    "test - this is a portion of the data that you keep hidden from your algorithm and only use it after the training takes places to compute some metrics that can give you a hint on how your algorithm behaves. For each item in you test dataset you predict its “value” using the built model and compare against the real “value”\n",
    "Now, back to the context of hyperparameter optimisation. If you run the same algorithm (train on training, evaluate on test) for multiple sets of hyperparameters and chose the model with the best “performance” on the test set you risk overfitting this test set. To avoid this problem of overfitting the test data, the training set is split once more into:\n",
    "\n",
    "actual training - a subset of the training set that is used to optimise the model\n",
    "validation - another subset of the training set that is used to evaluate the model performance for each run / set of hyperparameter values.\n",
    "Multiple training sessions are run on the actual training set, for various hyperparameter values and the models are evaluated agains the validation dataset. The model with the best performance is then chosen - remember that so far the algorithm has not yet seen the test data therefore there is no suspicion of overfitting it.\n",
    "\n",
    "After choosing the best model (and implicitly the values for the hyperparameters) this model is evaluated agains the test dataset and the performance is reported.\n",
    "\n",
    "Long story short: split your data into 3 subsets: training, validation, test. Train multiple variations of your model on the training dataset, chose the one with the best performance on the validation set and report how it generalise to the test set (important - the test set is kept hidden throughout the training process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 Model selection via cross validation. What part of the dataset (train, validation or test) is responsible for choosing the best model?\n",
    "\n",
    "ntioned the caveats in the train/test split method. In order to avoid this, \n",
    "we can perform something called cross validation. It’s very similar to train/test split, \n",
    "but it’s applied to more subsets. Meaning, we split our data into k subsets, and train \n",
    "on k-1 one of those subset. What we do is to hold the last subset for test. We’re able to do it \n",
    "for each of the subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What is parametric vs non-parametric model?\n",
    "Parametric models assume some finite set of parameters ✓. Given the parameters,\n",
    "future predictions, x, are independent of the observed data, D:\n",
    "P(x|✓, D) = P(x|✓)\n",
    "therefore ✓ capture everything there is to know about the data.\n",
    "• So the complexity of the model is bounded even if the amount of data is\n",
    "unbounded. This makes them not very flexible.\n",
    "• Non-parametric models assume that the data distribution cannot be defined in\n",
    "terms of such a finite set of parameters. But they can often be defined by\n",
    "assuming an infinite dimensional ✓. Usually we think of ✓ as a function.\n",
    "• The amount of information that ✓ can capture about the data D can grow as\n",
    "the amount of data grows. This makes them more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 \n",
    "You should normalize when the scale of a feature is irrelevant or misleading, and not normalize when the scale is meaningful.\n",
    "\n",
    "K-means considers Euclidean distance to be meaningful. If a feature has a big scale compared to another, but the first feature truly represents greater diversity, then clustering in that dimension should be penalized.\n",
    "\n",
    "In regression, as long as you have a bias it does not matter if you normalize or not since you are discovering an affine map, and the composition of a scaling transformation and an affine map is still affine.\n",
    "\n",
    "When there are learning rates involved, e.g. when you're doing gradient descent, the input scale effectively scales the gradients, which might require some kind of second order method to stabilize per-parameter learning rates. It's probably easier to normalize the inputs if it doesn't matter otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11  What is entropy? Which model uses this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy is a way of measuring the amount of impurity in a given set of data. \n",
    "\n",
    "The ID3 algorithm uses entropy to calculate the homogeneity of a sample. \n",
    "If the sample is completely homogeneous the entropy is zero and if the sample is \n",
    "equally divided it has the entropy of one.  Decision Trees use entropy and information gain to determine the best \n",
    "split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12: How is a random forest generated?\n",
    "Random Forest is a supervised learning algorithm. Like you can already see from it’s name, it creates a forest and makes it somehow random. The „forest“ it builds, is an ensemble of Decision Trees, most of the time trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result.\n",
    "\n",
    "To say it in simple words: Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
    "Random Forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.\n",
    "\n",
    "Therefore, in Random Forest, only a random subset of the features is taken into consideration by the algorithm for splitting a node. You can even make trees more random, by additionally using random thresholds for each feature rather than searching for the best possible thresholds (like a normal decision tree does)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13 The k-means algorithm is simple. It works from a given set of centers, which have traditionally been chosen randomly from among the points in the set X.\n",
    "\n",
    "The K-means loop iteratively performs two deterministic steps until (guaranteed) \n",
    "convergence: (1) pass over all data points and reassign them to their closest centers, \n",
    "then (2) recalculate the centers as the mean of the points \n",
    "assigned to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14:  What is a dendrogram?  \n",
    "Hierarchical Clustering / Dendrogram: Simple Definition, Examples\n",
    "Clustering > Hierarchical Clustering\n",
    "\n",
    "What is Hierarchical Clustering?\n",
    "Hierarchical clustering is where you build a cluster tree (a dendrogram) to represent data, where each group (or “node”) links to two or more successor groups. The groups are nested and organized as a tree, which ideally ends up as a meaningful classification scheme.\n",
    "\n",
    "Each node in the cluster tree contains a group of similar data; Nodes group on the graph next to other, similar nodes. Clusters at one level join with clusters in the next level up, using a degree of similarity; The process carries on until all nodes are in the tree, which gives a visual snapshot of the data contained in the whole set. The total number of clusters is not predetermined before you start the tree creation.\n",
    "\n",
    "\n",
    "\n",
    "A dendrogram is a type of tree diagram showing hierarchical clustering — relationships between similar sets of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15 What is linkage?  \n",
    "Performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
    "\n",
    "y must be a {n \\choose 2} sized vector where n is the number of original observations paired in the distance matrix. The behavior of this function is very similar to the MATLAB linkage function.\n",
    "\n",
    "A 4 by (n-1) matrix Z is returned. At the i-th iteration, clusters with indices Z[i, 0] and Z[i, 1] are combined to form cluster n + i. A cluster with an index less than n corresponds to one of the n original observations. The distance between clusters Z[i, 0] and Z[i, 1] is given by Z[i, 2]. The fourth value Z[i, 3] represents the number of original observations in the newly formed cluster.\n",
    "\n",
    "The following linkage methods are used to compute the distance d(s, t) between two clusters s and t. The algorithm begins with a forest of clusters that have yet to be used in the hierarchy being formed. When two clusters s and t from this forest are combined into a single cluster u, s and t are removed from the forest, and u is added to the forest. When only one cluster remains in the forest, the algorithm stops, and this cluster becomes the root.\n",
    "\n",
    "A distance matrix is maintained at each iteration. The d[i,j] entry corresponds to the distance between cluster i and j in the original forest.\n",
    "\n",
    "At each iteration, the algorithm must update the distance matrix to reflect the distance of the newly formed cluster u with the remaining clusters in the forest.\n",
    "\n",
    "Suppose there are |u| original observations u[0], \\ldots, u[|u|-1] in cluster u and |v| original objects v[0], \\ldots, v[|v|-1] in cluster v. Recall s and t are combined to form cluster u. Let v be any remaining cluster in the forest that is not u."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16 How does a recommender model work?\n",
    "Recommender Systems, as we said earlier, are an systems to recommend items to users. We have 2 kind of Recommender systems:\n",
    "\n",
    "User-based: the model find similarities between users\n",
    "Item-based: the model find similarities between items\n",
    "These systems are based on similarities, so the calculation of the correlation between data, so between users for the first case and items in the second case.\n",
    "The correlation is a numerical values between -1 and 1 that indicates how much two variables are related to each other. Correlation = 0 means no correlation, while >0 is positive correlation and <0 is negative correlation.  There are different methods to calculate the correlation coefficient, one of them in Pearsonmethod:\n",
    "So the correlation is the Covariance between two variables, X and Y , and the multiplication of their Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17  How can you reduce the number of features in a model?  bagging, correlation, feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18 What is a good use of PCA? \n",
    "PCA is an unsupervised linear dimensionality reduction algorithm to find a more meaningful \n",
    "basis or coordinate system for our data and works based on covariance matrix to find the \n",
    "strongest features if your samples .\n",
    "\n",
    "Its is used When we need to tackle the curse of dimensionality among data with linear \n",
    "relationships , i.e. where having too many dimensions (features) in your data causes noise \n",
    "and difficulties (it can be sound, picture or context). This specifically get worst when \n",
    "features have different scales (e.g. weight,length,area,speed, power, temperature,volume,\n",
    "                                time,cell number, etc. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19:  In general, how do neural networks \"learn\"?\n",
    "Artificial neural networks are organized into layers of parallel computing processes. For every processor in a layer, each of the number of inputs is multiplied by an originally established weight, resulting in what is called the internal value of the operation. This value is further changed by an originally created threshold value and sent to an activation function to map its output. The output of that function is then sent as the input for another layer, or as the final response of a network should the layer be the last. The weights and the threshold values are most commonly modified to produce the correct and most accurate value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 20 What is your favorite model? Why?  Random Forest because is can be used for both \n",
    "classificaiton and regression.  It is fairly simple yet can be very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
